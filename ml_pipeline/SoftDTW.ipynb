{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.optimizers import * \n",
    "from tensorflow.keras.losses import * "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmin(r, i, j, gamma=0.01):\n",
    "    if (i-1, j) in r:\n",
    "        a = -1.0 * r[(i-1, j)]   / gamma\n",
    "    else:\n",
    "        return tf.constant(float('inf'))\n",
    "    if (i-1,j-1) in r:\n",
    "        b = -1.0 * r[(i-1, j-1)] / gamma\n",
    "    else:\n",
    "        return tf.constant(float('inf'))\n",
    "    if (i,j-1) in r:\n",
    "        c = -1.0 * r[(i,   j-1)] / gamma\n",
    "    else:\n",
    "        return tf.constant(float('inf'))\n",
    "    max_val = tf.math.maximum(a, tf.math.maximum(b, c))\n",
    "    tmp  = tf.exp(a - max_val)\n",
    "    tmp += tf.exp(b - max_val)\n",
    "    tmp += tf.exp(c - max_val)\n",
    "    return -gamma * (tf.math.log(tmp) + max_val)\n",
    "    \n",
    "    \n",
    "def dtw(x, y, gamma=0.01):\n",
    "    n = x.shape[0]\n",
    "    m = y.shape[0]\n",
    "    band = int(((n + m) / 2) * 0.1)\n",
    "    r = {}\n",
    "    r[(0,0)] = tf.constant(0.0, dtype=tf.float32)\n",
    "    w = max(band, abs(n - m)) + 2\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(max(1, i - w), min(m + 1, i + w)):\n",
    "            dist = tf.sqrt(tf.reduce_sum(tf.square(x[i - 1, :] - y[j - 1, :])))\n",
    "            mini = softmin(r, i, j, gamma)   \n",
    "            dp   = dist + mini\n",
    "            r[(i, j)] = dist + mini         \n",
    "    return r[(n, m)]\n",
    "\n",
    "\n",
    "class TripletLoss(Loss):\n",
    "\n",
    "    def __init__(self, margin, latent):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        self.latent = latent\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        y_pred = tf.convert_to_tensor(y_pred)\n",
    "        anchor = y_pred[:, :, 0:self.latent]\n",
    "        pos    = y_pred[:, :, self.latent:2 * self.latent]\n",
    "        neg    = y_pred[:, :, 2 * self.latent:3 * self.latent]\n",
    "        pos_dist   = dtw(anchor[0, :, :], pos[0, :, :])         \n",
    "        neg_dist   = dtw(anchor[0, :, :], neg[0, :, :])        \n",
    "        basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), self.margin)  \n",
    "        loss       = tf.reduce_sum(tf.maximum(basic_loss, 0.0))   \n",
    "        print(\"Done\")\n",
    "        return loss\n",
    "\n",
    "    \n",
    "def encoder(in_shape, latent_dim, conv_params):\n",
    "    \"\"\"\n",
    "    A LSTM stack on top of a convolutional layer pooled in time.\n",
    "\n",
    "    See Figure [KOH4] Figure 2.\n",
    "\n",
    "    :param in_shape: the input shape to the model\n",
    "    :param latent_dim: embedding size of the model\n",
    "    :param conv_params: (conv_w, conv_h, filters)\n",
    "\n",
    "    :returns: a keras model\n",
    "    \"\"\"\n",
    "    kernel_size = (conv_params[0], conv_params[1])\n",
    "    n_filters = conv_params[2]\n",
    "    dft_dim = in_shape[1]\n",
    "    inp = Input(in_shape)\n",
    "    loc = Conv2D(n_filters, strides = (1, 1), kernel_size=kernel_size, activation='relu', padding='same')(inp) \n",
    "    loc = MaxPool2D(pool_size=(1, dft_dim))(loc) \n",
    "    loc = Reshape((in_shape[0], n_filters))(loc) \n",
    "    x   = Bidirectional(LSTM(latent_dim, return_sequences=True))(loc)\n",
    "    return Model(inputs =[inp], outputs=[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_pos = Input((32, 12, 1))\n",
    "in_neg = Input((32, 12, 1))\n",
    "in_anc = Input((32, 12, 1))\n",
    "\n",
    "e = encoder((32, 12, 1), 128, (8,8,64))\n",
    "\n",
    "out_pos = e(in_pos)\n",
    "out_neg = e(in_neg)\n",
    "out_anc = e(in_anc)\n",
    "conc    = Concatenate()([out_anc, out_neg, out_pos])\n",
    "\n",
    "model = Model(inputs=[in_anc, in_pos, in_neg], outputs=conc)  \n",
    "triplet_loss = TripletLoss(0.1, 128)\n",
    "model.compile(optimizer = 'adam', loss = triplet_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "a = np.random.uniform(0, 1, (1, 32, 12, 1))\n",
    "p = np.random.uniform(0, 1,(1, 32, 12, 1))\n",
    "z = np.random.uniform(0, 1, (1, 32, 12, 1))\n",
    "loss = model.train_on_batch(x=[a, p, z], y=np.zeros((1, 32, 768 )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
