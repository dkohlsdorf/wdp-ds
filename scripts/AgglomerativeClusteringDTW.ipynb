{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 -m pip install cython\n",
    "!python3 -m pip install PyMySQL\n",
    "!python3 -m pip install SQLAlchemy\n",
    "!python3 -m pip install google-cloud-storage\n",
    "!python3 -m pip install --upgrade --quiet scikit-sound\n",
    "!python3 -m pip install --upgrade --quiet pygame\n",
    "!python3 -m pip install scikit-learn==0.21rc2\n",
    "\n",
    "!sudo apt-get -y install ffmpeg\n",
    "!sudo apt-get -y install python3-pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import numpy as np\n",
    "\n",
    "from google.cloud import storage\n",
    "from numpy.fft import fft, ifft\n",
    "from sksound.sounds import Sound\n",
    "\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASSWORDS AND STUFF HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "   'user': db_user,\n",
    "   'pass': db_password,\n",
    "   'host': host,\n",
    "     'db': db_name\n",
    "}\n",
    "url = 'mysql+pymysql://{user}:{pass}@{host}/{db}'.format(**settings)  # 5432 is the default port\n",
    "db = sqlalchemy.create_engine(url)\n",
    "\n",
    "def run_query(query):\n",
    "    with db.connect() as conn:\n",
    "        rows = []\n",
    "        for row in conn.execute(query).fetchall():\n",
    "            rows.append(dict(row.items()))\n",
    "        return rows\n",
    "    \n",
    "regions = run_query(\"\"\"\n",
    "    SELECT \n",
    "        x.encoding, y.year, x.filename, x.start, x.stop\n",
    "    FROM \n",
    "        wdp_ds.not_silent x\n",
    "    JOIN \n",
    "        wdp_ds.encoding y \n",
    "    ON x.encoding = y.encoding;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = storage.Client.from_service_account_json('../secret.json')\n",
    "bucket = client.get_bucket('wdp-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions_by_file = {}\n",
    "for region in regions:\n",
    "    k = \"audio_files/{}/{}\".format(region['year'], region['filename'])\n",
    "    r = region\n",
    "    if k not in regions_by_file:\n",
    "        regions_by_file[k] = []\n",
    "    regions_by_file[k].append(r)\n",
    "print(len(regions_by_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highpass=25\n",
    "\n",
    "class StreamSpectrogram:\n",
    "    \n",
    "    def __init__(self, filename, starts, stops, win=32):\n",
    "        sound = Sound(filename)      \n",
    "        self.ranges = [x for x in zip(starts, stops)]\n",
    "        self.starts = starts\n",
    "        self.stops = stops\n",
    "        self.data  = sound.data\n",
    "        self.fs    = sound.rate\n",
    "        if len(self.data.shape) > 1:\n",
    "            self.data = self.data[:, 0]    \n",
    "        self.win = win\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.i = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):        \n",
    "        if self.i < len(self.ranges): \n",
    "            start, stop = self.ranges[self.i]\n",
    "            raw   = self.data[start:stop]\n",
    "            spec  = fwd_spectrogram(raw, win=512 + 2 * highpass)[:, 0:256] \n",
    "            t,d   = spec.shape\n",
    "            if t > self.win:\n",
    "                current = []        \n",
    "                for i in range(self.win, t + 1, self.win // 2):\n",
    "                    x      = np.reshape(spec[i - self.win:i], (self.win, d, 1))\n",
    "                    mu     = np.mean(x)\n",
    "                    std    = np.std(x) + 1.0\n",
    "                    window = (x - mu) / std\n",
    "                    current.append(window)\n",
    "                self.i += 1\n",
    "                return np.stack(current)\n",
    "            else:\n",
    "                self.i += 1\n",
    "                return None\n",
    "        else:\n",
    "            raise StopIteration                    \n",
    "            \n",
    "def fwd_spectrogram(audio, win=512, step=256):\n",
    "    '''\n",
    "    Compute the spectrogram of audio data\n",
    "\n",
    "    audio: one channel audio\n",
    "    win: window size for dft sliding window\n",
    "    step: step size for dft sliding windo\n",
    "    '''\n",
    "    spectrogram = []\n",
    "    hanning = np.hanning(win)\n",
    "    for i in range(win, len(audio), step):\n",
    "        start = win // 2\n",
    "        dft = np.abs(fft(audio[i - win: i] * hanning))[start:win]\n",
    "        spectrogram.append(dft)\n",
    "    return np.array(spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoder = load_model('../models/lstm_v3/v3.3/encoder.h5')\n",
    "idx     = list(regions_by_file.keys()) \n",
    "files   = {}\n",
    "latent  = [] \n",
    "for path in idx:    \n",
    "    with open(\"/tmp/audio.m4a\", \"wb\") as file_obj:\n",
    "        blob = bucket.blob(path)\n",
    "        blob.download_to_file(file_obj)\n",
    "    stream = StreamSpectrogram(\"/tmp/audio.m4a\", [r['start'] for r in regions_by_file[path]], [r['stop'] for r in regions_by_file[path]])\n",
    "    for i, region in enumerate(stream):\n",
    "        r = regions_by_file[path][i]\n",
    "        x = region\n",
    "        if x is not None:\n",
    "            h = encoder.predict(x)\n",
    "            latent.append([h, r])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "print(len(latent))\n",
    "pickle.dump( latent, open( \"data.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "latent = pickle.load( open( \"data.p\", \"rb\" ) )\n",
    "print(len(latent))\n",
    "x = [len(l[0]) for l in latent]\n",
    "plt.hist(x, bins=250)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "cimport numpy as np\n",
    "\n",
    "DTYPE = np.float\n",
    "\n",
    "cdef double euc(np.ndarray x, np.ndarray y):\n",
    "    return np.sqrt(np.sum(np.square(x - y)))\n",
    "\n",
    "cdef double min3(float x, float y, float z):\n",
    "    cdef float minimum = x\n",
    "    if y < minimum:\n",
    "        minimum = y\n",
    "    if z < minimum:\n",
    "        minimum = z\n",
    "    return minimum\n",
    "\n",
    "cdef double dtw(np.ndarray x, np.ndarray y):\n",
    "    cdef int n = x.shape[0]\n",
    "    cdef int m = y.shape[0]\n",
    "    cdef int w = max(round((n + m) / 4), abs(n - m) + 2)\n",
    "    \n",
    "    cdef np.ndarray dp = np.ones((n + 1, m + 1)) * float('inf')\n",
    "    cdef int i,j\n",
    "    dp[0,0] = 0.0\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(max(1, i-w), min(m + 1, i+w)):\n",
    "            dp[i][j] = euc(x[i - 1, :], y[j - 1, :]) + min3(\n",
    "                dp[i - 1][j - 1],\n",
    "                dp[i - 1][j    ],\n",
    "                dp[i    ][j - 1]\n",
    "            )\n",
    "    if np.isinf(dp[n][m]):\n",
    "        print('\\t\\tERROR: inf in warping')\n",
    "    return dp[n][m] / (n + m)\n",
    "\n",
    "latent = pickle.load( open( \"data.p\", \"rb\" ) )\n",
    "print(len(latent))\n",
    "n = len(latent)\n",
    "distances = np.ones((n, n)) * float('inf')\n",
    "for i, x in enumerate(latent):\n",
    "    if i % 10 == 0:\n",
    "        print(\"{} / {}\".format(i, n))\n",
    "    for j, y in enumerate(latent):\n",
    "        distances[i][j] = dtw(x[0], y[0])\n",
    "pickle.dump(distances, open( \"distances.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "latent    = pickle.load( open( \"data.p\", \"rb\" ) )\n",
    "distances = pickle.load( open( \"distances.p\", \"rb\" ) )\n",
    "\n",
    "f = [f for f in distances.flatten() if not np.isinf(f)]\n",
    "random.shuffle(f)\n",
    "th = np.percentile(f,30)\n",
    "\n",
    "plt.plot(sorted(f[0:1000]))\n",
    "plt.plot(np.ones(1000) * th)\n",
    "plt.show()\n",
    "print(th)\n",
    "\n",
    "for i in range(len(distances)):\n",
    "    for j in range(len(distances)):\n",
    "        if np.isinf(distances[i][j]):\n",
    "            distances[i][j] = distances[j][i]\n",
    "\n",
    "plt.imshow(distances, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(distances, open( \"distances_normal.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "latent = pickle.load( open( \"data.p\", \"rb\" ) )\n",
    "distances = pickle.load( open( \"distances_normal.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = AgglomerativeClustering(\n",
    "    n_clusters=None, \n",
    "    affinity='precomputed', \n",
    "    linkage='average', \n",
    "    distance_threshold=th\n",
    ")\n",
    "clusters = ac.fit_predict(distances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [[] for i in range(len(set(clusters)))]\n",
    "for i, cluster_id in enumerate(clusters):\n",
    "    latent[i][1:][0]['cluster'] = cluster_id\n",
    "    c[cluster_id].append(latent[i][1:][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clustering = [c for c in c if len(c) >= 2 and len(c) <= 100]\n",
    "pickle.dump(final_clustering, open( \"final_clustering.p\", \"wb\" ) )\n",
    "print(len(final_clustering))\n",
    "print([len(c) for c in final_clustering])\n",
    "print(sum([len(c) for c in final_clustering]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "480\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "480\n",
      "Cluster: 15\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "480\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "480\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "864\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "864\n",
      "Cluster: 44\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "864\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "864\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "864\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "864\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "864\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "864\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "864\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "864\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "1632\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "1632\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "1632\n",
      "Cluster: 3\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "1632\n",
      "Cluster: 32\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "1632\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "1632\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "1632\n",
      "Cluster: 3\n",
      "Cluster: 3\n",
      "Cluster: 5\n",
      "Cluster: 2\n",
      "Cluster: 4\n",
      "Cluster: 2\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "1632\n",
      "Cluster: 3\n",
      "Cluster: 2\n",
      "Cluster: 4\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 5\n",
      "Cluster: 4\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 4\n",
      "Cluster: 3\n",
      "Cluster: 3\n",
      "Cluster: 3\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 3\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "1632\n",
      "Cluster: 2\n",
      "Cluster: 12\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "1632\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 4\n",
      "Cluster: 3\n",
      "Cluster: 7\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "1632\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 4\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 3\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 3\n",
      "Cluster: 2\n",
      "Cluster: 5\n",
      "Cluster: 3\n",
      "Cluster: 5\n",
      "Cluster: 4\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 4\n",
      "Cluster: 2\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "1632\n",
      "Cluster: 2\n",
      "Cluster: 4\n",
      "Cluster: 3\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 6\n",
      "Cluster: 5\n",
      "Cluster: 4\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "1632\n",
      "Cluster: 3\n",
      "Cluster: 4\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "1632\n",
      "Cluster: 3\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 3\n",
      "Cluster: 2\n",
      "Cluster: 3\n",
      "Cluster: 2\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "1632\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 3\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "1632\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 2\n",
      "Cluster: 3\n",
      "Cluster: 8\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "1632\n",
      "Cluster: 3\n"
     ]
    }
   ],
   "source": [
    "from scipy.io import wavfile\n",
    "import sys\n",
    "from google.cloud import storage\n",
    "import pickle\n",
    "\n",
    "final_clustering = pickle.load(open( \"final_clustering.p\", \"rb\" ))\n",
    "\n",
    "print(len(final_clustering))\n",
    "print([len(c) for c in final_clustering])\n",
    "\n",
    "print(\"CLUSTERS: \", len(final_clustering))\n",
    "\n",
    "cache = {}\n",
    "client = storage.Client.from_service_account_json('../secret.json')\n",
    "bucket = client.get_bucket('wdp-data')\n",
    "\n",
    "cx = 0\n",
    "skip = 0\n",
    "for cluster in final_clustering:\n",
    "    if cx >= skip:\n",
    "        n = len(cluster)\n",
    "        print(\"Cluster: {}\".format(n))\n",
    "        audio = []\n",
    "        for i, region in enumerate(cluster):\n",
    "            path = \"audio_files/{}/{}\".format(region['year'], region['filename'])\n",
    "            if path in cache:\n",
    "                stream = cache[path]\n",
    "            else:                \n",
    "                with open(\"/tmp/audio.m4a\", \"wb\") as file_obj:\n",
    "                    blob = bucket.blob(path)\n",
    "                    blob.download_to_file(file_obj)\n",
    "                stream = StreamSpectrogram(\"/tmp/audio.m4a\", [region['start']], [region['stop']])\n",
    "                cache[path] = stream              \n",
    "                print(sys.getsizeof(cache))\n",
    "            spec = fwd_spectrogram(stream.data[region['start']:region['stop']])\n",
    "            audio.extend(stream.data[region['start']:region['stop']])\n",
    "            audio.extend(np.zeros(stream.fs // 10))    \n",
    "        audio = np.array(audio, dtype=audio[0].dtype)\n",
    "        wavfile.write('../data/results/cluster_{}.wav'.format(cx), stream.fs, audio)\n",
    "    cx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "max_id = run_query(\"\"\"\n",
    "    SELECT \n",
    "        max(id)\n",
    "    FROM \n",
    "        wdp_ds.clustering_results\n",
    "    \"\"\")\n",
    "if max_id[0]['max(id)'] is None:\n",
    "    max_id = 0\n",
    "else:\n",
    "    max_id = max_id[0]['max(id)']\n",
    "print(max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with db.connect() as conn:\n",
    "    id = 0\n",
    "    for cluster in final_clustering:\n",
    "        for r in cluster:\n",
    "            query = \"INSERT INTO wdp_ds.clustering_results VALUES({}, {}, '{}', {}, {}, '{}', {}, CURRENT_TIMESTAMP)\".format(\n",
    "                id, r['encoding'], r['filename'], r['start'], r['stop'], 'agglomerative_dtw_lstm_v2', r['cluster'])        \n",
    "            conn.execute(query)  \n",
    "            id += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
