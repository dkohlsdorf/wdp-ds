{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyMySQL in /home/jupyter/.local/lib/python3.5/site-packages\n",
      "Requirement already satisfied: SQLAlchemy in /usr/local/lib/python3.5/dist-packages\n",
      "Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.5/dist-packages\n",
      "Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.3 in /usr/local/lib/python3.5/dist-packages (from google-cloud-storage)\n",
      "Requirement already satisfied: google-auth>=1.2.0 in /usr/local/lib/python3.5/dist-packages (from google-cloud-storage)\n",
      "Requirement already satisfied: google-resumable-media!=0.4.0,<0.5dev,>=0.3.1 in /usr/local/lib/python3.5/dist-packages (from google-cloud-storage)\n",
      "Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.5/dist-packages (from google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.5/dist-packages (from google-auth>=1.2.0->google-cloud-storage)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.5/dist-packages (from google-auth>=1.2.0->google-cloud-storage)\n",
      "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from google-auth>=1.2.0->google-cloud-storage)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.5/dist-packages (from google-auth>=1.2.0->google-cloud-storage)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.5/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage)\n",
      "Requirement already satisfied: setuptools>=34.0.0 in /usr/local/lib/python3.5/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.5/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage)\n",
      "Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.5/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.5/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.5/dist-packages (from rsa>=3.1.4->google-auth>=1.2.0->google-cloud-storage)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.5/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.5/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.5/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.5/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.3->google-cloud-storage)\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:3.2.14-1~deb9u1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "python3-pymysql is already the newest version (0.7.10-1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install PyMySQL\n",
    "!python3 -m pip install SQLAlchemy\n",
    "!python3 -m pip install google-cloud-storage\n",
    "!python3 -m pip install --upgrade --quiet scikit-sound\n",
    "!python3 -m pip install --upgrade --quiet pygame\n",
    "!sudo apt-get -y install ffmpeg\n",
    "!sudo apt-get -y install python3-pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import sqlalchemy\n",
    "import numpy as np\n",
    "\n",
    "from google.cloud import storage\n",
    "from numpy.fft import fft, ifft\n",
    "from sksound.sounds import Sound\n",
    "\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "highpass=25\n",
    "\n",
    "class StreamSpectrogram:\n",
    "    \n",
    "    def __init__(self, filename, win=32):\n",
    "        sound = Sound(filename)        \n",
    "        self.batch = sound.rate * 60\n",
    "        self.data  = sound.data\n",
    "        if len(self.data.shape) > 1:\n",
    "            self.data = self.data[:, 0]    \n",
    "        self.win = win\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.i = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if (self.i + 1) * self.batch < len(self.data): \n",
    "            start = self.i       * self.batch\n",
    "            stop  = (self.i + 1) * self.batch\n",
    "            raw   = self.data[start:stop]\n",
    "            spec  = fwd_spectrogram(raw, win=512 + 2 * highpass)[:, 0:256] \n",
    "            t,d   = spec.shape\n",
    "            current = []        \n",
    "            for i in range(self.win, t, self.win // 2):\n",
    "                x      = np.reshape(spec[i - self.win:i], (self.win, d, 1))\n",
    "                mu     = np.mean(x)\n",
    "                std    = np.std(x) + 1.0\n",
    "                window = (x - mu) / std\n",
    "                current.append(window)\n",
    "            self.i += 1\n",
    "            return np.stack(current)\n",
    "        else:\n",
    "            raise StopIteration\n",
    "        \n",
    "def fwd_spectrogram(audio, win=512, step=256):\n",
    "    '''\n",
    "    Compute the spectrogram of audio data\n",
    "\n",
    "    audio: one channel audio\n",
    "    win: window size for dft sliding window\n",
    "    step: step size for dft sliding windo\n",
    "    '''\n",
    "    spectrogram = []\n",
    "    hanning = np.hanning(win)\n",
    "    for i in range(win, len(audio), step):\n",
    "        start = win // 2\n",
    "        dft = np.abs(fft(audio[i - win: i] * hanning))[start:win]\n",
    "        spectrogram.append(dft)\n",
    "    return np.array(spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASSWORDS AND STUFF HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "   'user': db_user,\n",
    "   'pass': db_password,\n",
    "   'host': host,\n",
    "     'db': db_name\n",
    "}\n",
    "url = 'mysql+pymysql://{user}:{pass}@{host}/{db}'.format(**settings)  # 5432 is the default port\n",
    "db = sqlalchemy.create_engine(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(query):\n",
    "    with db.connect() as conn:\n",
    "        rows = []\n",
    "        for row in conn.execute(query).fetchall():\n",
    "            rows.append(dict(row.items()))\n",
    "        return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = run_query(\"\"\"\n",
    "    SELECT \n",
    "        x.encoding, y.year, x.filename \n",
    "    FROM \n",
    "        wdp_ds.audio x \n",
    "    JOIN wdp_ds.encoding y ON x.encoding = y.encoding;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"audio_files/{}/{}\".format(file['year'], file['filename']) for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "max_id = run_query(\"\"\"\n",
    "    SELECT \n",
    "        max(id)\n",
    "    FROM \n",
    "        wdp_ds.not_silent\n",
    "    \"\"\")\n",
    "if max_id[0]['max(id)'] is None:\n",
    "    max_id = 0\n",
    "else:\n",
    "    max_id = max_id[0]['max(id)']\n",
    "print(max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_classifier = load_model('../models/lstm_v2/sil.h5')\n",
    "client = storage.Client.from_service_account_json('../secret.json')\n",
    "bucket = client.get_bucket('wdp-data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_files/2011/06101101.m4a {'filename': '06101101.m4a', 'year': 2011, 'encoding': 6101101}\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "6101101 :  []\n",
      "audio_files/2011/05111101.m4a {'filename': '05111101.m4a', 'year': 2011, 'encoding': 5111101}\n",
      "Infile converted from .m4a to \".wav\"\n",
      "data read in!\n",
      "\t\t\t\t(11248, 281)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_1 to have shape (32, 256, 1) but got array with shape (32, 29, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-dd8a210184a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mnot_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoise_classifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mnot_noise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m     return self._model_iteration(\n\u001b[1;32m    461\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m         steps=steps, callbacks=callbacks, **kwargs)\n\u001b[0m\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m           \u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m           \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m           distribution_strategy=strategy)\n\u001b[0m\u001b[1;32m    397\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m         steps=steps)\n\u001b[0m\u001b[1;32m    595\u001b[0m   adapter = adapter_cls(\n\u001b[1;32m    596\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[1;32m   2470\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2471\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2472\u001b[0;31m           exception_prefix='input')\n\u001b[0m\u001b[1;32m   2473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2474\u001b[0m     \u001b[0;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow_core/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    572\u001b[0m                              \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m                              str(data_shape))\n\u001b[0m\u001b[1;32m    575\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_1 to have shape (32, 256, 1) but got array with shape (32, 29, 1)"
     ]
    }
   ],
   "source": [
    "id = max_id + 1\n",
    "for path, file_dict in zip(paths, files):\n",
    "    print(path, file_dict)\n",
    "    with open(\"/tmp/audio.m4a\", \"wb\") as file_obj:\n",
    "        blob = bucket.blob(path)\n",
    "        blob.download_to_file(file_obj)\n",
    "    stream = StreamSpectrogram(\"/tmp/audio.m4a\")\n",
    "    not_noise = []\n",
    "    for x in stream:\n",
    "        y = noise_classifier.predict(x).flatten()\n",
    "        not_noise.extend([int(np.round(sample)) == 0 for sample in y])\n",
    "\n",
    "    regions = []\n",
    "    for i in range(0, len(not_noise)):\n",
    "        if not_noise[i]:\n",
    "            start = i * 16 * 256 \n",
    "            stop  = (i + 1) * 16 * 256 \n",
    "            if len(regions) > 0: \n",
    "                last  = regions[-1]\n",
    "                if start - last[1] < 48000 * 0.1:\n",
    "                    start       = regions[-1][0]\n",
    "                    regions[-1] = (start, stop)\n",
    "                else:\n",
    "                    regions.append((start, stop))\n",
    "            else:\n",
    "                regions.append((start, stop))\n",
    "    regions = [(start, stop) for start, stop in regions if stop - start > (16 * 256)]\n",
    "    if len(regions) > 0:\n",
    "        with db.connect() as conn:\n",
    "            for start, stop in regions:                \n",
    "                conn.execute(\"INSERT INTO wdp_ds.not_silent VALUES ({}, {}, '{}', {}, {})\".format(id, file_dict['encoding'], file_dict['filename'], start, stop))  \n",
    "                id += 1\n",
    "    print(file_dict['encoding'], \": \", regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
